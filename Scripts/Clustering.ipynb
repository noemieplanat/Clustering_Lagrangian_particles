{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4ba54f-9126-45dd-8e83-19eebad7d5a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T12:59:19.010505Z",
     "start_time": "2022-07-05T12:59:08.813310Z"
    }
   },
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import copy as cp\n",
    "import random\n",
    "import pyproj as proj\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import KernelPCA\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "from sklearn import cluster\n",
    "import wpca\n",
    "import pandas as pd\n",
    "import pickle \n",
    "from matplotlib.cm import get_cmap\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy as cr\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad15dd44-25bd-4b51-a2ed-f3bb3ae31d4d",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b124f77d-dcac-4c73-9d3e-906136e1b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUB_SETS = {'all':{'Subset_name':'/storage/nplanat/Glorys12_OP_journalier/NEW_RUN/Sub_samp_y_1993_-_2014_delta_1_100000_start_lon_0.nc',\\\n",
    "                  'init_year':1993,\\\n",
    "                  'final_year':2014,\\\n",
    "                   'delta_year':1,\\\n",
    "                  'N_particles':100000,\\\n",
    "                  'Subset_path':'/storage/nplanat/Glorys12_OP_journalier/NEW_RUN/'},\\\n",
    "           'weird_path_only':{'Subset_name':'/storage/nplanat/Glorys12_OP_journalier/WEIRD/Sub_samp_y_1996_-_1997_-delta_1_40000_start_lon_0.nc',\\\n",
    "                  'init_year':1996,\\\n",
    "                  'final_year':1997,\\\n",
    "                   'delta_year':1,\\\n",
    "                  'N_particles':40000,\\\n",
    "                  'Subset_path':'/storage/nplanat/Glorys12_OP_journalier/WEIRD/'},\\\n",
    "           'without_weird_path':{'Subset_name':None,\\\n",
    "                  'init_year':1993,\\\n",
    "                  'final_year':2014,\\\n",
    "                   'delta_year':1,\\\n",
    "                  'N_particles':100000,\\\n",
    "                  'Subset_path':'/storage/nplanat/Glorys12_OP_journalier/NO_WEIRD/'},\\\n",
    "           'all_short':{'Subset_name':'/storage/nplanat/Glorys12_OP_journalier/SHORT/Sub_samp_y_1993_-_2014_delta_1_100000_start_lon_0.nc',\\\n",
    "                  'init_year':1993,\\\n",
    "                  'final_year':2014,\\\n",
    "                   'delta_year':1,\\\n",
    "                  'N_particles':100000,\\\n",
    "                  'Subset_path':'/storage/nplanat/Glorys12_OP_journalier/SHORT/'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a667c31-0474-4400-b640-5fe5f8d3dc6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T12:59:38.503869Z",
     "start_time": "2022-07-05T12:59:38.500454Z"
    }
   },
   "outputs": [],
   "source": [
    "sel_subset = 'all_short'\n",
    "load_previous_run = True\n",
    "init_year = SUB_SETS[sel_subset]['init_year']\n",
    "final_year = SUB_SETS[sel_subset]['final_year']\n",
    "delta_year = SUB_SETS[sel_subset]['delta_year']\n",
    "Subset_name = SUB_SETS[sel_subset]['Subset_name']\n",
    "N_particles = SUB_SETS[sel_subset]['N_particles']\n",
    "Subset_path = SUB_SETS[sel_subset]['Subset_path']\n",
    "\n",
    "files = sorted(glob.glob('/storage/nplanat/Glorys12_OP_journalier/ADV_new_*'))\n",
    "files_m = sorted(glob.glob('/storage/nplanat/Glorys12_OP_journalier/ADV_m_200_*'))\n",
    "files_s = sorted(glob.glob('/storage/nplanat/Glorys12_OP_journalier/ADV_s_200_*'))\n",
    "\n",
    "length_days = 450\n",
    "path_save_prediction = Subset_path \n",
    "\n",
    "\n",
    "perctest = 0.1 \n",
    "perctrain = 0.8\n",
    "trans = True\n",
    "\n",
    "#PCA \n",
    "n_components = 0.9999999\n",
    "\n",
    "# Clustering\n",
    "init = 'k-means++'\n",
    "nmb_initialisations = 20  # number of initiatilisaton for the k-means++ \n",
    "max_iter = 500\n",
    "tol = 5e-4\n",
    "algorithm = 'full'\n",
    "verbose = 0 \n",
    "sample_weight = None\n",
    "n_split = 20  # number of iterations for convergence\n",
    "n_clusters = 20 #number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61b68a7-7a56-48f6-9b00-b7aa5c583a16",
   "metadata": {},
   "source": [
    "# Generate sub-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9feded90-4cf5-418c-8db6-fb1222802da2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T09:36:32.365677Z",
     "start_time": "2022-07-04T09:36:32.363245Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_short(ds): \n",
    "    l = [len(ds.sel(traj=i).lat.dropna(dim='obs')) for i in ds.traj]\n",
    "    keep = np.array([i for i in range(len(l)) if l[i] > 90]) # more than 3 months\n",
    "    ds2 = ds.sel(traj=xr.DataArray(keep)).rename_dims({'dim_0':'traj'}) \n",
    "    return ds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82d8a9fb-f3a7-4e68-a87e-0faa902872d5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T09:36:32.377190Z",
     "start_time": "2022-07-04T09:36:32.366612Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_traj_sub(year_init, year_final, delta_year, files, length_days):\n",
    "    dslist = []\n",
    "    for yr in range(year_init,year_final+1,delta_year):\n",
    "        print(yr)\n",
    "        filesS = files[np.where(np.array([int(files[i][-7:-3]) for i in range(len(files))]) == yr)[0][0]]\n",
    "        dsl = xr.open_dataset(filesS)\n",
    "        dsl = remove_short(dsl) # remove short trajectories -- to avoid memory errors\n",
    "        dsl = dsl.sel(obs = np.arange(length_days+366))\n",
    "        dslist.append(dsl)\n",
    "\n",
    "    \n",
    "    return dslist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2bd4644-e774-4fd0-9f60-f1b552bb5233",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T09:36:32.387727Z",
     "start_time": "2022-07-04T09:36:32.378078Z"
    }
   },
   "outputs": [],
   "source": [
    "def random_sample(N_particles, ds):\n",
    "    random.seed(4)\n",
    "    sample = random.sample(list(ds.traj.values), k=N_particles) \n",
    "    ds_samp = ds.sel(traj=xr.DataArray(sample)).rename_dims({'dim_0':'traj'})  # select the random particles\n",
    "    return ds_samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94a9d0f2-42cc-4cd6-89d4-7bc9f08e063e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T09:36:32.398732Z",
     "start_time": "2022-07-04T09:36:32.388465Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1993\n",
      "1994\n",
      "1995\n",
      "1996\n",
      "1997\n",
      "1998\n",
      "1999\n",
      "2000\n",
      "2001\n",
      "2002\n",
      "2003\n",
      "2004\n",
      "2005\n",
      "2006\n",
      "2007\n",
      "2008\n",
      "2009\n",
      "2010\n",
      "2011\n",
      "2012\n",
      "2013\n",
      "2014\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ds_samp = xr.open_dataset(Subset_name)\n",
    "except:\n",
    "    dslist = get_traj_sub(init_year, final_year, delta_year, files, length_days)\n",
    "    L_all = [x.compute() for x in dslist]\n",
    "    ds = xr.concat(L_all, dim='traj')\n",
    "    ds_samp = random_sample(N_particles, ds)\n",
    "    ds_samp.to_netcdf(Subset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e21cc2-1f8d-4777-b181-879d7d41fce3",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6af5e4ea-4c7c-4789-af00-295f62ee79ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T09:36:32.410758Z",
     "start_time": "2022-07-04T09:36:32.406600Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def split_sets(lats_all, lons_all, temps_all, sals_all, perctest, perctrain):\n",
    "    \n",
    "    s0, s1 = lats_all.shape\n",
    "    data = np.concatenate((lats_all,lons_all), axis=1)\n",
    "    \n",
    "    random.seed(4)\n",
    "    random.shuffle(data)\n",
    "    random.seed(4)\n",
    "    random.shuffle(temps_all)\n",
    "    random.seed(4)\n",
    "    random.shuffle(sals_all)\n",
    "\n",
    "    lats_test = data[:int(perctest*s0), 0:s1]\n",
    "    lons_test = data[:int(perctest*s0), s1:]\n",
    "\n",
    "    lats_train = data[int(perctest*s0):int(perctest*s0)+int(perctrain*s0), 0:s1]\n",
    "    lons_train = data[int(perctest*s0):int(perctest*s0)+int(perctrain*s0), s1:]\n",
    "\n",
    "    lats_valid = data[int(perctest*s0)+int(perctrain*s0):, 0:s1]\n",
    "    lons_valid = data[int(perctest*s0)+int(perctrain*s0):, s1:]\n",
    "\n",
    "    temps_test = temps_all[:int(perctest*s0),:]\n",
    "    sals_test = sals_all[:int(perctest*s0),:]\n",
    "    \n",
    "    temps_train =temps_all[int(perctest*s0):int(perctest*s0)+int(perctrain*s0),:]\n",
    "    sals_train = sals_all[int(perctest*s0):int(perctest*s0)+int(perctrain*s0),:]\n",
    "    \n",
    "    temps_valid = temps_all[int(perctest*s0)+int(perctrain*s0):,:]\n",
    "    sals_valid = sals_all[int(perctest*s0)+int(perctrain*s0):,:]\n",
    "    \n",
    "    return lats_test, lons_test, lats_train, lons_train, lats_valid, lons_valid, temps_train, temps_test, temps_valid, sals_train, sals_test, sals_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ec96b76-216c-4df8-91c2-8876a3218496",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T09:36:32.421579Z",
     "start_time": "2022-07-04T09:36:32.411647Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def lons_in_interval(start_lon, lons):\n",
    "    #this should shift all longitudes so it lies in start_lon + 360. \n",
    "    if start_lon<0 : \n",
    "        print('start_lons should be in [0-360[')\n",
    "        return None\n",
    "    else:\n",
    "        lons_shifted = lons\n",
    "        while np.any(lons_shifted<0):\n",
    "            lons_shifted[lons_shifted<0] = lons_shifted[lons_shifted<0]+360\n",
    "        lons_2 = cp.copy(lons_shifted)\n",
    "        lons_2[lons_shifted>=start_lon] = lons_shifted[lons_shifted>=start_lon] - start_lon\n",
    "        lons_2[lons_shifted<start_lon] = lons_shifted[lons_shifted<start_lon] - start_lon +360\n",
    "        return cp.copy(lons_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8b032176-7a82-4408-a03b-1c7ceaebdc3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T13:16:34.680697Z",
     "start_time": "2022-07-04T13:16:34.677621Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def extract(path, length_days) :\n",
    "    ds = xr.open_dataset(path)\n",
    "    ds = remove_short(ds)\n",
    "    lats = ds.lat[:,0:length_days].values\n",
    "    lons_i = ds.lon[:,0:length_days].values\n",
    "    lons = lons_in_interval(0, lons_i) # by default start_lon = 360\n",
    "    temps = ds.temperature[:,0:length_days].values\n",
    "    sal = ds.salinity[:,0:length_days].values\n",
    "    z = ds.z[:,0:length_days].values\n",
    "    date = ds.time[:,0].values\n",
    "    return lats, lons, temps, sal, date, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84aa2f8f-0d80-4def-ab35-10354bbbe284",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T09:36:32.441619Z",
     "start_time": "2022-07-04T09:36:32.432610Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def translate(lat, lon):\n",
    "    return lat-np.repeat(lat[:,0][:, np.newaxis], lat.shape[1], axis=1), lon-np.repeat(lon[:,0][:, np.newaxis], lon.shape[1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b309ce3-13c4-495c-8c7a-71125058a7e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T09:36:32.452176Z",
     "start_time": "2022-07-04T09:36:32.442617Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def Part_1_pre_processing_one_sort(t, la, lo):\n",
    "    if t == True :\n",
    "        lats_train_pp, lons_train_pp = translate(la, lo)\n",
    "\n",
    "    # Reshape\n",
    "    X_lats_lons_train = np.concatenate((lats_train_pp, lons_train_pp), axis = 1) #concatenation of features to make it in the correct shape\n",
    "    X_lats_lons_train[np.isnan(X_lats_lons_train)] = 0 \n",
    "   \n",
    "    return X_lats_lons_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61a425d4-f98e-4877-a07f-6eb053c66479",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T09:36:32.463071Z",
     "start_time": "2022-07-04T09:36:32.452895Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def Part_1_pre_processing(Dataset_path, length_days, perctest, perctrain, t):\n",
    "    lats_all,lons_all, temps_all, sals_all, date_all, depth_all = extract(Dataset_path, length_days)\n",
    "    lats_test, lons_test, lats_train, lons_train, lats_valid, lons_valid, temps_train, temps_test, temps_valid, sals_train, sals_test, sals_valid = split_sets(lats_all, lons_all,temps_all, sals_all, perctest, perctrain)\n",
    "\n",
    "    print('Pre_processing')\n",
    "    # PRE-PROCESSING\n",
    "    X_lats_lons_train = Part_1_pre_processing_one_sort(t, lats_train, lons_train)\n",
    "    X_lats_lons_test = Part_1_pre_processing_one_sort(t, lats_test, lons_test)\n",
    "    X_lats_lons_valid = Part_1_pre_processing_one_sort(t, lats_valid, lons_valid)\n",
    "    \n",
    "    return X_lats_lons_valid, X_lats_lons_test, X_lats_lons_train, lats_test, lons_test, lats_train, lons_train, lats_valid, lons_valid,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92ab2f01-5334-4dc8-8b91-68e45e5e5f81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T09:47:15.191250Z",
     "start_time": "2022-07-04T09:36:32.463954Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre_processing\n"
     ]
    }
   ],
   "source": [
    "X_lats_lons_valid, X_lats_lons_test, X_lats_lons_train, lats_test, lons_test, lats_train, lons_train, lats_valid, lons_valid = Part_1_pre_processing(Subset_name, length_days, perctest, perctrain, trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26916a3-7068-420c-85fa-f10ade208b00",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f390bba-a6fa-4b9a-ac36-2168ce8d852d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T09:47:15.194579Z",
     "start_time": "2022-07-04T09:47:15.192672Z"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n_components)#, copy=copy, whiten=whiten, svd_solver = svd_solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5e83c3fa-4077-4740-bac7-d684814c40d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T09:47:23.447468Z",
     "start_time": "2022-07-04T09:47:15.195319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of initial features was:  900\n",
      "The number of selected features is:  752\n",
      "The number of samples is:  80000\n",
      "The explained variance desired is: 0.9999999 %, the obtained variance explained is:  0.9999999017605582 %\n"
     ]
    }
   ],
   "source": [
    "X_reduced_train = pca.fit_transform(X_lats_lons_train)\n",
    "N_features_PCA = pca.n_features_\n",
    "N_samples_PCA = pca.n_samples_\n",
    "N_components_PCA = pca.n_components_\n",
    "Explained_variance_ratio_PCA = pca.explained_variance_ratio_\n",
    "print('The number of initial features was: ', N_features_PCA)\n",
    "print('The number of selected features is: ', N_components_PCA)\n",
    "print('The number of samples is: ', N_samples_PCA)\n",
    "print('The explained variance desired is:', n_components, '%, the obtained variance explained is: ', np.sum(Explained_variance_ratio_PCA), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a1ae8c55-4213-421b-9b99-d6ca97f177ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T09:47:23.626382Z",
     "start_time": "2022-07-04T09:47:23.448530Z"
    }
   },
   "outputs": [],
   "source": [
    "X_reduced_valid = pca.transform(X_lats_lons_valid)\n",
    "X_reduced_test  = pca.transform(X_lats_lons_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2aaf6dff-c66d-43d1-8e92-9361ac5ce5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save PCA : \n",
    "with open(Subset_path + 'pca.pkl', 'wb') as pickle_file:\n",
    "        pickle.dump(pca, pickle_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1537b0f4-9986-4ea0-8bb6-60f1340adeeb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9b531a5-9226-4fe2-aec8-797ee86a36d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T09:47:23.634034Z",
     "start_time": "2022-07-04T09:47:23.629291Z"
    },
    "code_folding": [
     0
    ],
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def kmeans(n_splits, X_reduced, n_clusters, init, nmb_initialisations, max_iter, tol, algorithm, verbose, sample_weight):\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True)\n",
    "    avg_silouhette = 0\n",
    "    i = 0\n",
    "    for train_index, val_index in kf.split(X_reduced):\n",
    "        print('fold nmb ', i)\n",
    "        i+=1\n",
    "        X_train = X_reduced[train_index,:]\n",
    "        k_means = cluster.KMeans(n_clusters=n_clusters, init=init, n_init = nmb_initialisations, max_iter = max_iter, tol = tol, algorithm = algorithm, verbose = verbose)\n",
    "        k_means.fit(X_train, sample_weight = sample_weight)\n",
    "        X_centers = k_means.cluster_centers_\n",
    "        a_temp = k_means.inertia_\n",
    "        if a_temp>avg_silouhette:\n",
    "            X_centered_memory = X_centers\n",
    "            k_means_memory = k_means\n",
    "            avg_silouhette = a_temp\n",
    "    \n",
    "    return X_centered_memory, k_means_memory, a_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f020c6c-5bdb-4f13-aeee-1846f6fdcb52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T10:09:49.106599Z",
     "start_time": "2022-07-04T09:47:23.635283Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold nmb  0\n",
      "fold nmb  1\n",
      "fold nmb  2\n",
      "fold nmb  3\n",
      "fold nmb  4\n",
      "fold nmb  5\n",
      "fold nmb  6\n",
      "fold nmb  7\n",
      "fold nmb  8\n",
      "fold nmb  9\n",
      "fold nmb  10\n",
      "fold nmb  11\n",
      "fold nmb  12\n",
      "fold nmb  13\n",
      "fold nmb  14\n",
      "fold nmb  15\n",
      "fold nmb  16\n",
      "fold nmb  17\n",
      "fold nmb  18\n",
      "fold nmb  19\n"
     ]
    }
   ],
   "source": [
    "Traj_centroids, kmeans, score = kmeans(n_split, X_reduced_train, n_clusters, init, nmb_initialisations, max_iter, tol, algorithm, verbose, sample_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e052fb27-e8d4-4873-8a7d-0712185b132b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# clustering\n",
    "np.save(Subset_path + 'Traj_centroids', Traj_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88ae89-c6b7-4d89-bde1-2ee66e59a90d",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fd72b49d-0e28-477c-b72f-41fddccbb8fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T10:09:49.110576Z",
     "start_time": "2022-07-04T10:09:49.107911Z"
    },
    "code_folding": [
     0
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(X_reduced_val, X_centers):\n",
    "    \n",
    "    X_reduced_val2 = np.repeat(X_reduced_val[:,:,np.newaxis], X_centers.shape[0], axis = 2)\n",
    "    X_centers2 = np.repeat(np.transpose(X_centers)[np.newaxis,:,:], X_reduced_val.shape[0], axis = 0)\n",
    "    \n",
    "    return np.argmin(np.linalg.norm(X_reduced_val2-X_centers2, axis=1), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1e1ff543-4737-4930-a47f-47147029a439",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T10:09:51.180219Z",
     "start_time": "2022-07-04T10:09:49.111397Z"
    }
   },
   "outputs": [],
   "source": [
    "Labels_valid = predict(X_reduced_valid, Traj_centroids)\n",
    "Labels_test = predict(X_reduced_test, Traj_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bbf5d9ce-df41-4463-988e-cd21be62bcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(path_save_prediction+'Labels_valid', Labels_valid)\n",
    "np.save(path_save_prediction+'Labels_test', Labels_test)\n",
    "np.save(path_save_prediction+'lats_test', lats_test)\n",
    "np.save(path_save_prediction+'lons_test', lons_test)\n",
    "np.save(path_save_prediction+'lats_train', lats_train)\n",
    "np.save(path_save_prediction+'lons_train', lons_train)\n",
    "np.save(path_save_prediction+'lats_valid', lats_valid)\n",
    "np.save(path_save_prediction+'lons_valid', lons_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6070c4-2ab0-4da3-b68c-24b0790c7ed0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Generate Lats-Lons  all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fa0e360b-14cc-451b-81c3-4c6d241d4bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dates(path, length_days) :\n",
    "    ds = xr.open_dataset(path)\n",
    "    ds = remove_short(ds)\n",
    "    date = ds.time[:,0].values\n",
    "    return date\n",
    "def extract_all_dates_year(yr, length_days, all_filles) :\n",
    "    file = all_filles[np.where(np.array([int(all_filles[i][-7:-3]) for i in range(len(all_filles))]) == yr)[0][0]]\n",
    "    date = extract_dates(file, length_days)    \n",
    "    return date\n",
    "def extract_all_per_year(yr, length_days, all_filles) :\n",
    "    file = all_filles[np.where(np.array([int(all_filles[i][-7:-3]) for i in range(len(all_filles))]) == yr)[0][0]]\n",
    "    lats, lons, temps, sal, date, depth = extract(file, length_days)    \n",
    "    return lats, lons, temps, sal, date, depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7bbac300-b543-4dc7-bdf6-457821a50cf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try : \n",
    "    np.load('/storage/nplanat/Glorys12_OP_journalier/lats_lons/times_2013.npy', allow_pickle = True)\n",
    "except:\n",
    "    for yr in range(1993,2014):\n",
    "        print('Year', yr)\n",
    "        lats_all, lons_all, temps_all, sals_all, date_all, prof_all = extract_all_per_year(yr, length_days, files)\n",
    "        np.save('/storage/nplanat/Glorys12_OP_journalier/lats_lons/lats_%i'%yr, lats_all)\n",
    "        np.save('/storage/nplanat/Glorys12_OP_journalier/lats_lons/depths_%i'%yr, prof_all)\n",
    "        np.save('/storage/nplanat/Glorys12_OP_journalier/lats_lons/temps_%i'%yr, temps_all)\n",
    "        np.save('/storage/nplanat/Glorys12_OP_journalier/lats_lons/sals_%i'%yr, sals_all)\n",
    "        np.save('/storage/nplanat/Glorys12_OP_journalier/lats_lons/lons_%i'%yr, lons_all)\n",
    "        date_all = extract_all_dates_year(yr, length_days, files)\n",
    "        np.save('/storage/nplanat/Glorys12_OP_journalier/lats_lons/times_%i'%yr, date_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8b993a8-e922-43a4-8927-b79cc97b01f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try : \n",
    "    np.load('/storage/nplanat/Glorys12_OP_journalier/lats_lons/m_times_2013.npy', allow_pickle = True)\n",
    "except:\n",
    "    for yr in range(1993,2014):\n",
    "        print('Year', yr)\n",
    "        lats_all, lons_all, temps_all, sals_all, date_all, prof_all = extract_all_per_year(yr, length_days, files_m)\n",
    "        np.save('/storage/nplanat/Glorys12_OP_journalier/lats_lons/m_lats_%i'%yr, lats_all)\n",
    "        np.save('/storage/nplanat/Glorys12_OP_journalier/lats_lons/m_depths_%i'%yr, prof_all)\n",
    "        np.save('/storage/nplanat/Glorys12_OP_journalier/lats_lons/m_temps_%i'%yr, temps_all)\n",
    "        np.save('/storage/nplanat/Glorys12_OP_journalier/lats_lons/m_sals_%i'%yr, sals_all)\n",
    "        np.save('/storage/nplanat/Glorys12_OP_journalier/lats_lons/m_lons_%i'%yr, lons_all)\n",
    "        date_all = extract_all_dates_year(yr, length_days, files_m)\n",
    "        np.save('/storage/nplanat/Glorys12_OP_journalier/lats_lons/m_times_%i'%yr, date_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ee020c7-29fe-4cac-bc10-ba900d44be19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try : \n",
    "    np.load('/storage/nplanat/Glorys12_OP_journalier/lats_lons/s_times_2013.npy', allow_pickle = True)\n",
    "except:\n",
    "    for yr in range(1993,2014):\n",
    "        print('Year', yr)\n",
    "        lats_all, lons_all, temps_all, sals_all, date_all, prof_all = extract_all_per_year(yr, length_days, files_s)\n",
    "        np.save('/storage/nplanat/Glorys12_OP_journalier/lats_lons/s_lats_%i'%yr, lats_all)\n",
    "        np.save('/storage/nplanat/Glorys12_OP_journalier/lats_lons/s_depths_%i'%yr, prof_all)\n",
    "        np.save('/storage/nplanat/Glorys12_OP_journalier/lats_lons/s_temps_%i'%yr, temps_all)\n",
    "        np.save('/storage/nplanat/Glorys12_OP_journalier/lats_lons/s_sals_%i'%yr, sals_all)\n",
    "        np.save('/storage/nplanat/Glorys12_OP_journalier/lats_lons/s_lons_%i'%yr, lons_all)\n",
    "        date_all = extract_all_dates_year(yr, length_days, files_s)\n",
    "        np.save('/storage/nplanat/Glorys12_OP_journalier/lats_lons/s_times_%i'%yr, date_all)"
   ]
  }
 ],
 "metadata": {
  "hide_input": true,
  "kernelspec": {
   "display_name": "Python(analysis)",
   "language": "python",
   "name": "analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
